{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1tSBEPYInBA"
   },
   "source": [
    "# Tessarine Valued CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FgmMIBA-BTGi"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as kr\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import datasets, models \n",
    "from tensorflow.keras import layers, activations, initializers, regularizers\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from tensorflow.python.keras.engine.input_spec import InputSpec\n",
    "from functools import partial\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.datasets import cifar10\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.callbacks import LearningRateScheduler, ReduceLROnPlateau\n",
    "from tensorflow.python.ops import variables as tf_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NVhb5L0CBS_1",
    "outputId": "f2b2f0e1-5ab8-46fa-9909-2e0d73aff661"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 2s 0us/step\n",
      "170508288/170498071 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(Xtr_cifar, ytr_cifar), (Xte_cifar, yte_cifar) = cifar10.load_data()\n",
    "Xtr_cifar = Xtr_cifar / 255\n",
    "Xte_cifar = Xte_cifar / 255\n",
    "\n",
    "Xtr_cifar -= np.mean(Xtr_cifar, axis=0)\n",
    "Xte_cifar -= np.mean(Xtr_cifar, axis=0)\n",
    "\n",
    "n_classes = 10\n",
    "ytr_cifar = kr.utils.to_categorical(ytr_cifar, num_classes=n_classes)\n",
    "yte_cifar = kr.utils.to_categorical(yte_cifar, num_classes=n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "aUbpdbwk2zG_"
   },
   "outputs": [],
   "source": [
    "def learning_rate(epoch):\n",
    "    lr = 1e-2\n",
    "    if epoch < 151 and epoch > 9:\n",
    "        lr *= 10.\n",
    "    elif epoch > 199:\n",
    "        lr /= 10.\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fiPPfWnNkbs-"
   },
   "outputs": [],
   "source": [
    "def _compute_fans(shape):\n",
    "    \"\"\"Computes the number of input and output units for a weight shape.\n",
    "    Args:\n",
    "        shape: Integer shape tuple or TF tensor shape.\n",
    "    Returns:\n",
    "        A tuple of integer scalars (fan_in, fan_out).\n",
    "\n",
    "    Extracted from tensorflow/keras/initializers. Available at\n",
    "    https://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/python/keras/initializers/initializers_v2.py\n",
    "    \"\"\"\n",
    "\n",
    "    if len(shape) < 1:  # Just to avoid errors for constants.\n",
    "        fan_in = fan_out = 1\n",
    "    elif len(shape) == 1:\n",
    "        fan_in = fan_out = shape[0]\n",
    "    elif len(shape) == 2:\n",
    "        fan_in = shape[0]\n",
    "        fan_out = shape[1]\n",
    "    else:\n",
    "        # Assuming convolution kernels (2D, 3D, or more).\n",
    "        # kernel shape: (..., input_depth, depth)\n",
    "        receptive_field_size = 1\n",
    "        for dim in shape[:-2]:\n",
    "            receptive_field_size *= dim\n",
    "        fan_in = shape[-2] * receptive_field_size\n",
    "        fan_out = shape[-1] * receptive_field_size\n",
    "    return int(fan_in), int(fan_out)\n",
    "\n",
    "\n",
    "class Hypercomplex4DInitializer(initializers.Initializer):\n",
    "    \"\"\"\n",
    "    Computes initialization based on quaternion variance.\n",
    "    Options: he uniform, he normal, glorot uniform, glorot normal.\n",
    "    References:\n",
    "    [1] He, K., Zhang, X., Ren, S., and Sun, J. (2015b).  Delving deep into rectifiers: Surpassing human-level performance on imagenet classification.\n",
    "    [2] Glorot, X. and Bengio, Y. (2010).  Understanding the difficulty of training deep feedforward neural networks.  \n",
    "    In Teh, Y. W. and Titterington, M., editors, Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,\n",
    "    volume 9 of Proceedings of Machine Learning Research, pages 249–256, Chia Laguna Resort, Sardinia, Italy. PMLR.\n",
    "    \"\"\"\n",
    "    def __init__(self, criterion='he', distribution='uniform', seed=31337):\n",
    "        self.criterion = criterion\n",
    "        self.distribution = distribution\n",
    "        self.seed = seed\n",
    "\n",
    "    def __call__(self, shape, dtype):\n",
    "        fan_in, fan_out = _compute_fans(shape)\n",
    "\n",
    "        if self.criterion == 'he':\n",
    "            std = 1. / np.sqrt(2 * fan_in)\n",
    "        elif self.criterion == 'glorot':\n",
    "            std = 1. / np.sqrt(2 * (fan_in + fan_out))\n",
    "        else:\n",
    "            raise ValueError(\"Chosen criterion was not identified.\")\n",
    "\n",
    "        if self.distribution == 'normal':\n",
    "            return tf.random.normal(shape, mean=0, stddev=std, dtype=dtype, seed=self.seed)\n",
    "        elif self.distribution == 'uniform':\n",
    "            lim = std * np.sqrt(3)\n",
    "            return tf.random.uniform(shape, minval=-lim, maxval=lim, dtype=dtype, seed=self.seed)\n",
    "        else:\n",
    "            raise ValueError(\"Chosen distribution was not identified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jwyY935HA7Nm"
   },
   "outputs": [],
   "source": [
    "class TessConv2D(layers.Layer):\n",
    "    \"\"\"\n",
    "    Tessarine valued 2D convolution layer.\n",
    "    References:\n",
    "    [1] Trabelsi, C., Bilaniuk, O., Serdyuk, D., Subramanian, S., Santos, J. F., Mehri, S., Rostamzadeh, N., Bengio, Y., and Pal, C. J. (2017). Deep complex networks.\n",
    "    [2] Gaudet, C. and Maida, A. (2017). Deep quaternion networks.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 filters, \n",
    "                 kernel_size, \n",
    "                 strides=1, \n",
    "                 padding='SAME',\n",
    "                 use_bias=False,\n",
    "                 activation=None,\n",
    "                 initializer=Hypercomplex4DInitializer,\n",
    "                 data_format=None,\n",
    "                 kernel_regularizer=1e-3):\n",
    "        super(TessConv2D, self).__init__()\n",
    "        self.filters = filters \n",
    "        self.kernel_size = kernel_size\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "        self.use_bias = use_bias\n",
    "        self.activation = activations.get(activation)\n",
    "        self.initializer = initializer\n",
    "        self.data_format = data_format\n",
    "        if kernel_regularizer is not None:\n",
    "            self.kernel_regularizer = kr.regularizers.l2(kernel_regularizer)\n",
    "        else:\n",
    "            self.kernel_regularizer = kernel_regularizer\n",
    "\n",
    "    def _get_channel_axis(self):\n",
    "        if self.data_format == 'channels_first':\n",
    "            raise ValueError('TessConv2d is designed only for channels_last. '\n",
    "                             'The input has been changed to channels last!')\n",
    "        else:\n",
    "            return -1\n",
    "    \n",
    "    def _get_input_channel(self, input_shape):\n",
    "        channel_axis = self._get_channel_axis()\n",
    "        if input_shape.dims[channel_axis].value is None:\n",
    "            raise ValueError('The channel dimension of the inputs '\n",
    "                             'should be defined. Found `None`.')\n",
    "        return int(input_shape[channel_axis])\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_shape = tensor_shape.TensorShape(input_shape)\n",
    "        input_channel = self._get_input_channel(input_shape)\n",
    "        if input_channel % 4 != 0:\n",
    "            raise ValueError('The number of input channels must be divisible by 4.')\n",
    "    \n",
    "        input_dim = input_channel // 4\n",
    "        kernel_shape = self.kernel_size + (input_dim, self.filters)\n",
    "        \n",
    "        self.f0 = self.add_weight(\n",
    "            name='real_kernel',\n",
    "            shape=kernel_shape,\n",
    "            initializer=self.initializer,\n",
    "            trainable=True,\n",
    "            regularizer=self.kernel_regularizer\n",
    "        )\n",
    "        self.f1 = self.add_weight(\n",
    "            name='imag_i_kernel',\n",
    "            shape=kernel_shape,\n",
    "            initializer=self.initializer,\n",
    "            trainable=True,\n",
    "            regularizer=self.kernel_regularizer\n",
    "        )\n",
    "        self.f2 = self.add_weight(\n",
    "            name='imag_j_kernel',\n",
    "            shape=kernel_shape,\n",
    "            initializer=self.initializer,\n",
    "            trainable=True,\n",
    "            regularizer=self.kernel_regularizer\n",
    "        )\n",
    "        self.f3 = self.add_weight(\n",
    "            name='imag_k_kernel',\n",
    "            shape=kernel_shape,\n",
    "            initializer=self.initializer,\n",
    "            trainable=True,\n",
    "            regularizer=self.kernel_regularizer\n",
    "        )\n",
    "        \n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(\n",
    "                name='bias',\n",
    "                shape=(4*self.filters,),\n",
    "                initializer=\"zeros\",\n",
    "                trainable=True,\n",
    "                dtype=self.dtype)\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def call(self, inputs):\n",
    "        F_r = tf.concat([ self.f0,-self.f1, self.f2,-self.f3],axis=2)\n",
    "        F_i = tf.concat([ self.f1, self.f0, self.f3, self.f2],axis=2)\n",
    "        F_j = tf.concat([ self.f2,-self.f3, self.f0,-self.f1],axis=2)\n",
    "        F_k = tf.concat([ self.f3, self.f2, self.f1, self.f0],axis=2)\n",
    "               \n",
    "        y_r = tf.nn.conv2d(inputs, F_r, strides=self.strides, padding=self.padding)\n",
    "        y_i = tf.nn.conv2d(inputs, F_i, strides=self.strides, padding=self.padding)\n",
    "        y_j = tf.nn.conv2d(inputs, F_j, strides=self.strides, padding=self.padding)\n",
    "        y_k = tf.nn.conv2d(inputs, F_k, strides=self.strides, padding=self.padding)\n",
    "        \n",
    "        outputs = tf.concat([y_r, y_i, y_j, y_k],axis=3)\n",
    "        \n",
    "        if self.use_bias:\n",
    "            outputs = tf.nn.bias_add(outputs,self.bias)\n",
    "            \n",
    "        if self.activation is not None:\n",
    "            outputs = self.activation(outputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "bs2GhJfMWD1U"
   },
   "outputs": [],
   "source": [
    "DefaultConvTess = partial(TessConv2D, kernel_size=(3,3), strides=1, padding=\"SAME\", kernel_regularizer=1e-3, use_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "OWswgBqAs8Xo"
   },
   "outputs": [],
   "source": [
    "def diag_init(shape, dtype=None):\n",
    "    return tf.ones(shape) / 2.\n",
    "\n",
    "class Hypercomplex4DBNActivation(layers.Layer):\n",
    "    \"\"\"\n",
    "    Batch Normalization for tessarines and quaternions.\n",
    "    Based on matrix whitening. Decorrelates each component of tessarine/quaternion.\n",
    "    Includes activation: can be placed before, after or in the middle of BN.\n",
    "    References:\n",
    "    [1] Ioffe, S. and Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift.\n",
    "    [2] Kessy, A., Lewin, A., and Strimmer, K. (2018). Optimal whitening and decorrelation. The American Statistician, 72(4):309–314.\n",
    "    [3] Trabelsi, C., Bilaniuk, O., Serdyuk, D., Subramanian, S., Santos, J. F., Mehri, S., Ros-tamzadeh, N., Bengio, Y., and Pal, C. J. (2017). Deep complex networks.\n",
    "    [4] Gaudet, C. and Maida, A. (2017). Deep quaternion networks.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 center=True,\n",
    "                 scale=True,\n",
    "                 momentum=0.9,\n",
    "                 beta_init='zeros',\n",
    "                 gam_diag_init='diag_init',\n",
    "                 gam_off_init='zeros',\n",
    "                 mov_mean_init='zeros',\n",
    "                 mov_var_init='diag_init',\n",
    "                 mov_cov_init='zeros',\n",
    "                 beta_reg=None,\n",
    "                 gam_diag_reg=None,\n",
    "                 gam_off_reg=None,\n",
    "                 activation=\"elu\",\n",
    "                 activation_position=\"after\",\n",
    "                 epsilon=1e-6,\n",
    "                 **kwargs):\n",
    "        super(Hypercomplex4DBNActivation, self).__init__(**kwargs)\n",
    "        self.center = center\n",
    "        self.scale = scale\n",
    "        self.momentum = momentum\n",
    "        self.beta_init = initializers.get(beta_init)\n",
    "\n",
    "        if gam_diag_init == 'diag_init':\n",
    "            self.gam_diag_init = diag_init\n",
    "        else:\n",
    "            self.gam_diag_init = initializers.get(gam_diag_init)\n",
    "\n",
    "        self.gam_off_init = initializers.get(gam_off_init)\n",
    "        self.mov_mean_init = initializers.get(mov_mean_init)\n",
    "\n",
    "        if mov_var_init == 'diag_init':\n",
    "            self.mov_var_init = diag_init\n",
    "        else:    \n",
    "            self.mov_var_init = initializers.get(mov_var_init)\n",
    "            \n",
    "        self.mov_cov_init = initializers.get(mov_cov_init)\n",
    "        self.beta_reg = regularizers.get(beta_reg)\n",
    "        self.gam_diag_reg = regularizers.get(gam_diag_reg)\n",
    "        self.gam_off_reg = regularizers.get(gam_off_reg)\n",
    "        self.activation = activations.get(activation)\n",
    "        self.activation_position = activation_position\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_dim = input_shape[-1] // 4\n",
    "        vars_shape = [input_dim, 1]\n",
    "        gamma_shape = (input_dim,)\n",
    "\n",
    "        if self.scale:\n",
    "            self.mov_Vrr = self.add_weight(shape=vars_shape,\n",
    "                                           initializer=self.mov_var_init,\n",
    "                                           trainable=False,\n",
    "                                           name=\"mov_Vrr\")\n",
    "            self.mov_Vri = self.add_weight(shape=vars_shape,\n",
    "                                           initializer=self.mov_cov_init,\n",
    "                                           trainable=False,\n",
    "                                           name=\"mov_Vri\")\n",
    "            self.mov_Vrj = self.add_weight(shape=vars_shape,\n",
    "                                           initializer=self.mov_cov_init,\n",
    "                                           trainable=False,\n",
    "                                           name=\"mov_Vrj\")\n",
    "            self.mov_Vrk = self.add_weight(shape=vars_shape,\n",
    "                                           initializer=self.mov_cov_init,\n",
    "                                           trainable=False,\n",
    "                                           name=\"mov_Vrk\")\n",
    "            self.mov_Vii = self.add_weight(shape=vars_shape,\n",
    "                                           initializer=self.mov_var_init,\n",
    "                                           trainable=False,\n",
    "                                           name=\"mov_Vii\")\n",
    "            self.mov_Vij = self.add_weight(shape=vars_shape,\n",
    "                                           initializer=self.mov_cov_init,\n",
    "                                           trainable=False,\n",
    "                                           name=\"mov_Vij\")\n",
    "            self.mov_Vik = self.add_weight(shape=vars_shape,\n",
    "                                           initializer=self.mov_cov_init,\n",
    "                                           trainable=False,\n",
    "                                           name=\"mov_Vik\")\n",
    "            self.mov_Vjj = self.add_weight(shape=vars_shape,\n",
    "                                           initializer=self.mov_var_init,\n",
    "                                           trainable=False,\n",
    "                                           name=\"mov_Vjj\")\n",
    "            self.mov_Vjk = self.add_weight(shape=vars_shape,\n",
    "                                           initializer=self.mov_cov_init,\n",
    "                                           trainable=False,\n",
    "                                           name=\"mov_Vjk\")\n",
    "            self.mov_Vkk = self.add_weight(shape=vars_shape,\n",
    "                                           initializer=self.mov_var_init,\n",
    "                                           trainable=False,\n",
    "                                           name=\"mov_Vkk\")\n",
    "\n",
    "            self.gam_rr = self.add_weight(shape=gamma_shape,\n",
    "                                          initializer=self.gam_diag_init,\n",
    "                                          regularizer=self.gam_diag_reg,\n",
    "                                          name=\"gam_rr\")\n",
    "            self.gam_ri = self.add_weight(shape=gamma_shape,\n",
    "                                          initializer=self.gam_off_init,\n",
    "                                          regularizer=self.gam_off_reg,\n",
    "                                          name=\"gam_ri\")\n",
    "            self.gam_rj = self.add_weight(shape=gamma_shape,\n",
    "                                          initializer=self.gam_off_init,\n",
    "                                          regularizer=self.gam_off_reg,\n",
    "                                          name=\"gam_rj\")\n",
    "            self.gam_rk = self.add_weight(shape=gamma_shape,\n",
    "                                          initializer=self.gam_off_init,\n",
    "                                          regularizer=self.gam_off_reg,\n",
    "                                          name=\"gam_rk\")\n",
    "            self.gam_ii = self.add_weight(shape=gamma_shape,\n",
    "                                          initializer=self.gam_diag_init,\n",
    "                                          regularizer=self.gam_diag_reg,\n",
    "                                          name=\"gam_ii\")\n",
    "            self.gam_ij = self.add_weight(shape=gamma_shape,\n",
    "                                          initializer=self.gam_off_init,\n",
    "                                          regularizer=self.gam_off_reg,\n",
    "                                          name=\"gam_ij\")\n",
    "            self.gam_ik = self.add_weight(shape=gamma_shape,\n",
    "                                          initializer=self.gam_off_init,\n",
    "                                          regularizer=self.gam_off_reg,\n",
    "                                          name=\"gam_ik\")\n",
    "            self.gam_jj = self.add_weight(shape=gamma_shape,\n",
    "                                          initializer=self.gam_diag_init,\n",
    "                                          regularizer=self.gam_diag_reg,\n",
    "                                          name=\"gam_jj\")\n",
    "            self.gam_jk = self.add_weight(shape=gamma_shape,\n",
    "                                          initializer=self.gam_off_init,\n",
    "                                          regularizer=self.gam_off_reg,\n",
    "                                          name=\"gam_jk\")\n",
    "            self.gam_kk = self.add_weight(shape=gamma_shape,\n",
    "                                          initializer=self.gam_diag_init,\n",
    "                                          regularizer=self.gam_diag_reg,\n",
    "                                          name=\"gam_kk\")\n",
    "        else:\n",
    "            self.mov_Vrr = None\n",
    "            self.mov_Vri = None\n",
    "            self.mov_Vrj = None\n",
    "            self.mov_Vrk = None\n",
    "            self.mov_Vii = None\n",
    "            self.mov_Vij = None\n",
    "            self.mov_Vik = None\n",
    "            self.mov_Vjj = None\n",
    "            self.mov_Vjk = None\n",
    "            self.mov_Vkk = None\n",
    "            self.gam_rr = None\n",
    "            self.gam_ri = None\n",
    "            self.gam_rj = None\n",
    "            self.gam_rk = None\n",
    "            self.gam_ii = None\n",
    "            self.gam_ij = None\n",
    "            self.gam_ik = None\n",
    "            self.gam_jj = None\n",
    "            self.gam_jk = None\n",
    "            self.gam_kk = None\n",
    "\n",
    "        if self.center:\n",
    "            self.beta = self.add_weight(shape=(1, 1, 1, input_shape[-1]),\n",
    "                                        initializer=self.beta_init,\n",
    "                                        regularizer=self.beta_reg,\n",
    "                                        name=\"beta\")\n",
    "            self.mov_mean = self.add_weight(shape=(1, 1, 1, input_shape[-1]),\n",
    "                                            initializer=self.mov_mean_init,\n",
    "                                            trainable=False,\n",
    "                                            name=\"mov_mean\")\n",
    "        else:\n",
    "            self.beta = None\n",
    "            self.mov_mean = None\n",
    "\n",
    "    def _compute_variances(self, centered_r, centered_i, centered_j, centered_k, input_dim):\n",
    "        Vrr = kr.backend.mean(\n",
    "            centered_r ** 2,\n",
    "            axis=[0, 1, 2]\n",
    "        ) + self.epsilon\n",
    "\n",
    "        Vri = kr.backend.mean(\n",
    "            centered_r * centered_i,\n",
    "            axis=[0, 1, 2]\n",
    "        )\n",
    "\n",
    "        Vrj = kr.backend.mean(\n",
    "            centered_r * centered_j,\n",
    "            axis=[0, 1, 2]\n",
    "        )\n",
    "\n",
    "        Vrk = kr.backend.mean(\n",
    "            centered_r * centered_k,\n",
    "            axis=[0, 1, 2]\n",
    "        )\n",
    "\n",
    "        Vii = kr.backend.mean(\n",
    "            centered_i ** 2,\n",
    "            axis=[0, 1, 2]\n",
    "        ) + self.epsilon\n",
    "\n",
    "        Vij = kr.backend.mean(\n",
    "            centered_i * centered_j,\n",
    "            axis=[0, 1, 2]\n",
    "        )\n",
    "\n",
    "        Vik = kr.backend.mean(\n",
    "            centered_i * centered_k,\n",
    "            axis=[0, 1, 2]\n",
    "        )\n",
    "\n",
    "        Vjj = kr.backend.mean(\n",
    "            centered_j ** 2,\n",
    "            axis=[0, 1, 2]\n",
    "        ) + self.epsilon\n",
    "\n",
    "        Vjk = kr.backend.mean(\n",
    "            centered_j * centered_k,\n",
    "            axis=[0, 1, 2]\n",
    "        )\n",
    "\n",
    "        Vkk = kr.backend.mean(\n",
    "            centered_k ** 2,\n",
    "            axis=[0, 1, 2]\n",
    "        ) + self.epsilon\n",
    "\n",
    "        pars_shape = [input_dim, 1]\n",
    "        Vrr = tf.reshape(Vrr, pars_shape)\n",
    "        Vri = tf.reshape(Vri, pars_shape)\n",
    "        Vrj = tf.reshape(Vrj, pars_shape)\n",
    "        Vrk = tf.reshape(Vrk, pars_shape)\n",
    "        Vii = tf.reshape(Vii, pars_shape)\n",
    "        Vij = tf.reshape(Vij, pars_shape)\n",
    "        Vik = tf.reshape(Vik, pars_shape)\n",
    "        Vjj = tf.reshape(Vjj, pars_shape)\n",
    "        Vjk = tf.reshape(Vjk, pars_shape)\n",
    "        Vkk = tf.reshape(Vkk, pars_shape)\n",
    "\n",
    "        return Vrr, Vri, Vrj, Vrk, Vii, Vij, Vik, Vjj, Vjk, Vkk\n",
    "    \n",
    "    def _moving_exponential_update(self, var, value):\n",
    "        decay = 1 - self.momentum\n",
    "        var.assign_sub(var * decay)\n",
    "        var.assign_add(value * decay)\n",
    "\n",
    "    def _update_moving_parameters(self, mean, Vrr, Vri, Vrj, Vrk, Vii, Vij, Vik, Vjj, Vjk, Vkk):\n",
    "        if self.center:\n",
    "            self._moving_exponential_update(self.mov_mean, mean)\n",
    "\n",
    "        if self.scale:\n",
    "            self._moving_exponential_update(self.mov_Vrr, Vrr)\n",
    "            self._moving_exponential_update(self.mov_Vri, Vri)\n",
    "            self._moving_exponential_update(self.mov_Vrj, Vrj)\n",
    "            self._moving_exponential_update(self.mov_Vrk, Vrk)\n",
    "            self._moving_exponential_update(self.mov_Vii, Vii)\n",
    "            self._moving_exponential_update(self.mov_Vij, Vij)\n",
    "            self._moving_exponential_update(self.mov_Vik, Vik)\n",
    "            self._moving_exponential_update(self.mov_Vjj, Vjj)\n",
    "            self._moving_exponential_update(self.mov_Vjk, Vjk)\n",
    "            self._moving_exponential_update(self.mov_Vkk, Vkk)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        if (not self.center) or (not self.scale):\n",
    "            raise ValueError(\"Batch Normalization should scale or center.\")\n",
    "\n",
    "        input_shape = kr.backend.int_shape(inputs)\n",
    "        input_dim = input_shape[-1] // 4\n",
    "\n",
    "        # Activation before\n",
    "        if self.activation_position == \"before\":\n",
    "            output = self.activation(inputs)\n",
    "        else:\n",
    "            output = inputs\n",
    "\n",
    "        if training in {0, False}:\n",
    "            mean = self.mov_mean\n",
    "            centered = output - mean\n",
    "\n",
    "            if self.scale:\n",
    "                centered_r = centered[:, :, :, :input_dim]\n",
    "                centered_i = centered[:, :, :, input_dim:input_dim * 2]\n",
    "                centered_j = centered[:, :, :, input_dim * 2:input_dim * 3]\n",
    "                centered_k = centered[:, :, :, input_dim * 3:]\n",
    "\n",
    "                Vrr = self.mov_Vrr\n",
    "                Vri = self.mov_Vri\n",
    "                Vrj = self.mov_Vrj\n",
    "                Vrk = self.mov_Vrk\n",
    "                Vii = self.mov_Vii\n",
    "                Vij = self.mov_Vij\n",
    "                Vik = self.mov_Vik\n",
    "                Vjj = self.mov_Vjj\n",
    "                Vjk = self.mov_Vjk\n",
    "                Vkk = self.mov_Vkk\n",
    "        else:\n",
    "            # mean and centering\n",
    "            mean = kr.backend.mean(output, axis=[0, 1, 2])\n",
    "            mean = kr.backend.reshape(mean, [1, 1, 1, input_dim * 4])\n",
    "            centered = output - mean\n",
    "\n",
    "            if self.scale:\n",
    "                centered_r = centered[:, :, :, :input_dim]\n",
    "                centered_i = centered[:, :, :, input_dim:input_dim * 2]\n",
    "                centered_j = centered[:, :, :, input_dim * 2:input_dim * 3]\n",
    "                centered_k = centered[:, :, :, input_dim * 3:]\n",
    "\n",
    "                Vrr, Vri, Vrj, Vrk, Vii, Vij, Vik, Vjj, Vjk, Vkk = self._compute_variances(centered_r, centered_i, centered_j, centered_k, input_dim)\n",
    "            else:\n",
    "                Vrr, Vri, Vrj, Vrk, Vii, Vij, Vik, Vjj, Vjk, Vkk = [None for i in range(10)]  \n",
    "\n",
    "            self._update_moving_parameters(mean, Vrr, Vri, Vrj, Vrk, Vii, Vij, Vik, Vjj, Vjk, Vkk)\n",
    "\n",
    "        if self.scale:\n",
    "            var_reshape = [input_dim, 1, 4]\n",
    "            # covariance matrix\n",
    "            V = tf.concat([[tf.reshape(tf.concat([Vrr, Vri, Vrj, Vrk], axis=1), var_reshape)],\n",
    "                           [tf.reshape(tf.concat([Vri, Vii, Vij, Vik], axis=1), var_reshape)],\n",
    "                           [tf.reshape(tf.concat([Vrj, Vij, Vjj, Vjk], axis=1), var_reshape)],\n",
    "                           [tf.reshape(tf.concat([Vrk, Vik, Vjk, Vkk], axis=1), var_reshape)]], axis=2)\n",
    "\n",
    "            # Whitening\n",
    "            R = tf.reshape(tf.linalg.cholesky(V), [input_dim, 4, 4])\n",
    "            W = tf.linalg.inv(tf.transpose(R, perm=[0,2,1]))\n",
    "\n",
    "            Wrr = W[:,0,0]\n",
    "            Wri = W[:,0,1]\n",
    "            Wrj = W[:,0,2]\n",
    "            Wrk = W[:,0,3]\n",
    "            Wii = W[:,1,1]\n",
    "            Wij = W[:,1,2]\n",
    "            Wik = W[:,1,3]\n",
    "            Wjj = W[:,2,2]\n",
    "            Wjk = W[:,2,3]\n",
    "            Wkk = W[:,3,3]\n",
    "\n",
    "            output_r = centered_r * Wrr\n",
    "            output_i = centered_r * Wri + centered_i * Wii\n",
    "            output_j = centered_r * Wrj + centered_i * Wij + centered_j * Wjj\n",
    "            output_k = centered_r * Wrk + centered_i * Wik + centered_j * Wjk + centered_k * Wkk\n",
    "\n",
    "            if self.activation_position == \"middle\":\n",
    "                output_r = self.activation(output_r)\n",
    "                output_i = self.activation(output_i)\n",
    "                output_j = self.activation(output_j)\n",
    "                output_k = self.activation(output_k)\n",
    "\n",
    "            out_r = output_r * self.gam_rr\n",
    "            out_i = output_r * self.gam_ri + output_i * self.gam_ii\n",
    "            out_j = output_r * self.gam_rj + output_i * self.gam_ij + output_j * self.gam_jj\n",
    "            out_k = output_r * self.gam_rk + output_i * self.gam_ik + output_j * self.gam_jk + output_k * self.gam_kk\n",
    "            output = tf.concat([out_r, out_i, out_j, out_k], axis=-1)\n",
    "        else:\n",
    "            output = centered\n",
    "            if self.activation_position == \"middle\":\n",
    "                output = self.activation(output)\n",
    "\n",
    "        if self.center:\n",
    "            output = output + self.beta\n",
    "\n",
    "        if self.activation_position == \"after\":\n",
    "            output = self.activation(output)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "69xSlwXXV6TF"
   },
   "outputs": [],
   "source": [
    "class TessResidualUnit(layers.Layer):\n",
    "    \"\"\" \n",
    "    Tessarine valued residual unit.\n",
    "    References:\n",
    "    [1] He, K., Zhang, X., Ren, S., and Sun, J. (2015).  Deep residual learning for image recog-nition.\n",
    "    [2] He, K., Zhang, X., Ren, S., and Sun, J. (2016).  Identity mappings in deep residual net-works.\n",
    "    \"\"\"\n",
    "    def __init__(self, filters, strides=1, activation=\"elu\", activation_position='after', **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.activation = kr.activations.get(activation)\n",
    "        self.main_layers = [\n",
    "            DefaultConvTess(filters, strides=strides),\n",
    "            Hypercomplex4DBNActivation(activation=activation, activation_position=activation_position),\n",
    "            DefaultConvTess(filters),\n",
    "            Hypercomplex4DBNActivation(activation=activation, activation_position=activation_position)]\n",
    "        self.skip_layers = []\n",
    "        if strides > 1:\n",
    "            self.skip_layers = [\n",
    "                DefaultConvTess(filters, kernel_size=(1,1), strides=strides),\n",
    "                Hypercomplex4DBNActivation(activation=None, activation_position='no_activation')]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.main_layers:\n",
    "            Z = layer(Z)\n",
    "        skip_Z = inputs\n",
    "        for layer in self.skip_layers:\n",
    "            skip_Z = layer(skip_Z)\n",
    "        return self.activation(Z + skip_Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O22ktPu7Xmbs",
    "outputId": "6905c901-a22c-4a55-e720-b7ab92761c02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.01\n"
     ]
    }
   ],
   "source": [
    "activation = \"elu\"\n",
    "\n",
    "tess_resnet = kr.models.Sequential()\n",
    "tess_resnet.add(DefaultConvTess(6, kernel_size=(3,3), strides=1))\n",
    "prev_filters = 6\n",
    "for filters in [6] * 3 + [12] * 2 + [24] * 2:\n",
    "    strides = 1 if filters == prev_filters else 2\n",
    "    tess_resnet.add(TessResidualUnit(filters, strides=strides, activation=activation))\n",
    "    prev_filters = filters\n",
    "tess_resnet.add(kr.layers.GlobalAvgPool2D())\n",
    "tess_resnet.add(kr.layers.Flatten())\n",
    "tess_resnet.add(kr.layers.Dense(10, activation=\"softmax\"))\n",
    "tess_resnet.compile(loss=\"categorical_crossentropy\", optimizer=kr.optimizers.SGD(learning_rate=learning_rate(0)), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "w1mB4Cvd3EaW"
   },
   "outputs": [],
   "source": [
    "lr_scheduler = LearningRateScheduler(learning_rate)\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
    "                               cooldown=0,\n",
    "                               patience=5,\n",
    "                               min_lr=1e-4)\n",
    "callbacks = [lr_reducer,lr_scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "gDSel8db3XEE"
   },
   "outputs": [],
   "source": [
    "Xtr_rgb = np.append(np.zeros([Xtr_cifar.shape[0],32,32,1]),Xtr_cifar,axis=3)\n",
    "Xte_rgb = np.append(np.zeros([Xte_cifar.shape[0],32,32,1]),Xte_cifar,axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "bPeKP5t23XBI"
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(width_shift_range=0.125, height_shift_range=0.125, horizontal_flip=True)\n",
    "datagen.fit(Xtr_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_DOZO9vmXmY3",
    "outputId": "ace27832-da47-43da-9e1c-62de6014a659"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 133s 238ms/step - loss: 2.3377 - accuracy: 0.3125 - val_loss: 2.6772 - val_accuracy: 0.2074\n",
      "Epoch 2/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 2.0791 - accuracy: 0.3997 - val_loss: 2.7237 - val_accuracy: 0.1476\n",
      "Epoch 3/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 90s 229ms/step - loss: 1.9789 - accuracy: 0.4362 - val_loss: 2.8793 - val_accuracy: 0.1783\n",
      "Epoch 4/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 89s 226ms/step - loss: 1.9045 - accuracy: 0.4660 - val_loss: 2.5306 - val_accuracy: 0.2793\n",
      "Epoch 5/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 88s 224ms/step - loss: 1.8459 - accuracy: 0.4826 - val_loss: 2.5480 - val_accuracy: 0.2585\n",
      "Epoch 6/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 1.8024 - accuracy: 0.4981 - val_loss: 2.4139 - val_accuracy: 0.3021\n",
      "Epoch 7/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 1.7570 - accuracy: 0.5117 - val_loss: 3.0704 - val_accuracy: 0.1875\n",
      "Epoch 8/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 88s 226ms/step - loss: 1.7188 - accuracy: 0.5233 - val_loss: 2.3131 - val_accuracy: 0.3145\n",
      "Epoch 9/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 90s 229ms/step - loss: 1.6815 - accuracy: 0.5382 - val_loss: 2.4022 - val_accuracy: 0.3006\n",
      "Epoch 10/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 1.6443 - accuracy: 0.5480 - val_loss: 2.2713 - val_accuracy: 0.3356\n",
      "Epoch 11/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 226ms/step - loss: 1.7873 - accuracy: 0.4907 - val_loss: 2.2474 - val_accuracy: 0.3554\n",
      "Epoch 12/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 229ms/step - loss: 1.5457 - accuracy: 0.5620 - val_loss: 2.7837 - val_accuracy: 0.3004\n",
      "Epoch 13/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 90s 229ms/step - loss: 1.4242 - accuracy: 0.5957 - val_loss: 2.1606 - val_accuracy: 0.3549\n",
      "Epoch 14/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 88s 226ms/step - loss: 1.3329 - accuracy: 0.6229 - val_loss: 1.8227 - val_accuracy: 0.4373\n",
      "Epoch 15/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 88s 226ms/step - loss: 1.2596 - accuracy: 0.6397 - val_loss: 1.9170 - val_accuracy: 0.4590\n",
      "Epoch 16/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 90s 229ms/step - loss: 1.2003 - accuracy: 0.6538 - val_loss: 1.6848 - val_accuracy: 0.4837\n",
      "Epoch 17/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 90s 230ms/step - loss: 1.1438 - accuracy: 0.6691 - val_loss: 2.4487 - val_accuracy: 0.3722\n",
      "Epoch 18/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 90s 229ms/step - loss: 1.0963 - accuracy: 0.6834 - val_loss: 1.6774 - val_accuracy: 0.4989\n",
      "Epoch 19/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 88s 226ms/step - loss: 1.0504 - accuracy: 0.6980 - val_loss: 1.4978 - val_accuracy: 0.5390\n",
      "Epoch 20/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 229ms/step - loss: 1.0078 - accuracy: 0.7116 - val_loss: 1.2715 - val_accuracy: 0.6257\n",
      "Epoch 21/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 90s 229ms/step - loss: 0.9693 - accuracy: 0.7223 - val_loss: 1.3678 - val_accuracy: 0.6036\n",
      "Epoch 22/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 227ms/step - loss: 0.9390 - accuracy: 0.7307 - val_loss: 1.4751 - val_accuracy: 0.5678\n",
      "Epoch 23/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 226ms/step - loss: 0.9124 - accuracy: 0.7402 - val_loss: 1.8737 - val_accuracy: 0.4859\n",
      "Epoch 24/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 227ms/step - loss: 0.8863 - accuracy: 0.7464 - val_loss: 1.5679 - val_accuracy: 0.5747\n",
      "Epoch 25/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 226ms/step - loss: 0.8626 - accuracy: 0.7559 - val_loss: 1.5093 - val_accuracy: 0.5831\n",
      "Epoch 26/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.8401 - accuracy: 0.7639 - val_loss: 1.0228 - val_accuracy: 0.7071\n",
      "Epoch 27/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 226ms/step - loss: 0.8190 - accuracy: 0.7701 - val_loss: 1.1649 - val_accuracy: 0.6542\n",
      "Epoch 28/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 90s 229ms/step - loss: 0.8036 - accuracy: 0.7732 - val_loss: 1.0196 - val_accuracy: 0.7016\n",
      "Epoch 29/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 88s 224ms/step - loss: 0.7895 - accuracy: 0.7789 - val_loss: 1.4640 - val_accuracy: 0.6058\n",
      "Epoch 30/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 227ms/step - loss: 0.7756 - accuracy: 0.7855 - val_loss: 1.1355 - val_accuracy: 0.6700\n",
      "Epoch 31/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.7647 - accuracy: 0.7890 - val_loss: 0.9994 - val_accuracy: 0.7160\n",
      "Epoch 32/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.7531 - accuracy: 0.7902 - val_loss: 1.2997 - val_accuracy: 0.6365\n",
      "Epoch 33/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 227ms/step - loss: 0.7395 - accuracy: 0.7954 - val_loss: 1.0418 - val_accuracy: 0.7064\n",
      "Epoch 34/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 227ms/step - loss: 0.7311 - accuracy: 0.7988 - val_loss: 0.8476 - val_accuracy: 0.7616\n",
      "Epoch 35/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.7214 - accuracy: 0.8018 - val_loss: 1.2149 - val_accuracy: 0.6588\n",
      "Epoch 36/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 229ms/step - loss: 0.7118 - accuracy: 0.8053 - val_loss: 1.1486 - val_accuracy: 0.6696\n",
      "Epoch 37/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 88s 224ms/step - loss: 0.7104 - accuracy: 0.8051 - val_loss: 1.1825 - val_accuracy: 0.6593\n",
      "Epoch 38/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 88s 226ms/step - loss: 0.7013 - accuracy: 0.8098 - val_loss: 1.1886 - val_accuracy: 0.6623\n",
      "Epoch 39/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 227ms/step - loss: 0.6904 - accuracy: 0.8134 - val_loss: 0.9809 - val_accuracy: 0.7227\n",
      "Epoch 40/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 88s 225ms/step - loss: 0.6828 - accuracy: 0.8154 - val_loss: 0.9954 - val_accuracy: 0.7235\n",
      "Epoch 41/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 227ms/step - loss: 0.6779 - accuracy: 0.8167 - val_loss: 0.9593 - val_accuracy: 0.7383\n",
      "Epoch 42/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.6712 - accuracy: 0.8203 - val_loss: 1.0603 - val_accuracy: 0.7036\n",
      "Epoch 43/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 87s 223ms/step - loss: 0.6687 - accuracy: 0.8206 - val_loss: 1.0689 - val_accuracy: 0.7038\n",
      "Epoch 44/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 88s 225ms/step - loss: 0.6591 - accuracy: 0.8269 - val_loss: 1.0092 - val_accuracy: 0.7160\n",
      "Epoch 45/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 227ms/step - loss: 0.6540 - accuracy: 0.8259 - val_loss: 0.9425 - val_accuracy: 0.7425\n",
      "Epoch 46/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 227ms/step - loss: 0.6490 - accuracy: 0.8290 - val_loss: 1.1267 - val_accuracy: 0.6747\n",
      "Epoch 47/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.6552 - accuracy: 0.8259 - val_loss: 0.8239 - val_accuracy: 0.7718\n",
      "Epoch 48/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 88s 226ms/step - loss: 0.6472 - accuracy: 0.8305 - val_loss: 0.9805 - val_accuracy: 0.7235\n",
      "Epoch 49/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 90s 229ms/step - loss: 0.6407 - accuracy: 0.8318 - val_loss: 1.2421 - val_accuracy: 0.6462\n",
      "Epoch 50/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 227ms/step - loss: 0.6410 - accuracy: 0.8318 - val_loss: 0.8336 - val_accuracy: 0.7760\n",
      "Epoch 51/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 227ms/step - loss: 0.6374 - accuracy: 0.8323 - val_loss: 1.2620 - val_accuracy: 0.6473\n",
      "Epoch 52/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 88s 225ms/step - loss: 0.6307 - accuracy: 0.8371 - val_loss: 1.0056 - val_accuracy: 0.7143\n",
      "Epoch 53/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 88s 225ms/step - loss: 0.6258 - accuracy: 0.8369 - val_loss: 0.8070 - val_accuracy: 0.7786\n",
      "Epoch 54/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 227ms/step - loss: 0.6271 - accuracy: 0.8370 - val_loss: 0.9200 - val_accuracy: 0.7425\n",
      "Epoch 55/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 226ms/step - loss: 0.6225 - accuracy: 0.8373 - val_loss: 0.9550 - val_accuracy: 0.7355\n",
      "Epoch 56/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 88s 225ms/step - loss: 0.6181 - accuracy: 0.8402 - val_loss: 0.9366 - val_accuracy: 0.7424\n",
      "Epoch 57/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.6162 - accuracy: 0.8412 - val_loss: 1.1404 - val_accuracy: 0.6866\n",
      "Epoch 58/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 88s 226ms/step - loss: 0.6176 - accuracy: 0.8399 - val_loss: 0.7988 - val_accuracy: 0.7795\n",
      "Epoch 59/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 227ms/step - loss: 0.6134 - accuracy: 0.8429 - val_loss: 0.9154 - val_accuracy: 0.7399\n",
      "Epoch 60/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.6075 - accuracy: 0.8438 - val_loss: 0.9405 - val_accuracy: 0.7362\n",
      "Epoch 61/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 90s 229ms/step - loss: 0.6039 - accuracy: 0.8456 - val_loss: 0.9256 - val_accuracy: 0.7478\n",
      "Epoch 62/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 88s 226ms/step - loss: 0.5991 - accuracy: 0.8487 - val_loss: 0.9397 - val_accuracy: 0.7465\n",
      "Epoch 63/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 88s 225ms/step - loss: 0.6015 - accuracy: 0.8460 - val_loss: 1.0479 - val_accuracy: 0.7115\n",
      "Epoch 64/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 227ms/step - loss: 0.6046 - accuracy: 0.8452 - val_loss: 1.0819 - val_accuracy: 0.7182\n",
      "Epoch 65/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 88s 226ms/step - loss: 0.5899 - accuracy: 0.8514 - val_loss: 1.0027 - val_accuracy: 0.7282\n",
      "Epoch 66/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.5950 - accuracy: 0.8487 - val_loss: 0.9841 - val_accuracy: 0.7231\n",
      "Epoch 67/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 227ms/step - loss: 0.5997 - accuracy: 0.8476 - val_loss: 0.9933 - val_accuracy: 0.7347\n",
      "Epoch 68/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 88s 226ms/step - loss: 0.5940 - accuracy: 0.8488 - val_loss: 0.9026 - val_accuracy: 0.7616\n",
      "Epoch 69/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 227ms/step - loss: 0.5908 - accuracy: 0.8493 - val_loss: 1.3238 - val_accuracy: 0.6382\n",
      "Epoch 70/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 227ms/step - loss: 0.5856 - accuracy: 0.8514 - val_loss: 0.8760 - val_accuracy: 0.7617\n",
      "Epoch 71/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 88s 224ms/step - loss: 0.5797 - accuracy: 0.8537 - val_loss: 0.8440 - val_accuracy: 0.7709\n",
      "Epoch 72/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.5812 - accuracy: 0.8562 - val_loss: 1.0342 - val_accuracy: 0.7186\n",
      "Epoch 73/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 227ms/step - loss: 0.5817 - accuracy: 0.8543 - val_loss: 1.1928 - val_accuracy: 0.6938\n",
      "Epoch 74/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 91s 232ms/step - loss: 0.5798 - accuracy: 0.8544 - val_loss: 0.8702 - val_accuracy: 0.7666\n",
      "Epoch 75/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 227ms/step - loss: 0.5782 - accuracy: 0.8556 - val_loss: 1.1499 - val_accuracy: 0.6904\n",
      "Epoch 76/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 90s 229ms/step - loss: 0.5764 - accuracy: 0.8553 - val_loss: 0.7605 - val_accuracy: 0.7992\n",
      "Epoch 77/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 88s 226ms/step - loss: 0.5755 - accuracy: 0.8574 - val_loss: 0.8178 - val_accuracy: 0.7825\n",
      "Epoch 78/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.5687 - accuracy: 0.8604 - val_loss: 0.8428 - val_accuracy: 0.7751\n",
      "Epoch 79/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 226ms/step - loss: 0.5774 - accuracy: 0.8563 - val_loss: 1.0332 - val_accuracy: 0.7213\n",
      "Epoch 80/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.5722 - accuracy: 0.8579 - val_loss: 0.8161 - val_accuracy: 0.7795\n",
      "Epoch 81/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 88s 226ms/step - loss: 0.5687 - accuracy: 0.8606 - val_loss: 0.8453 - val_accuracy: 0.7735\n",
      "Epoch 82/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 227ms/step - loss: 0.5674 - accuracy: 0.8595 - val_loss: 1.1193 - val_accuracy: 0.7093\n",
      "Epoch 83/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 227ms/step - loss: 0.5686 - accuracy: 0.8595 - val_loss: 0.8287 - val_accuracy: 0.7787\n",
      "Epoch 84/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.5628 - accuracy: 0.8619 - val_loss: 1.0649 - val_accuracy: 0.7216\n",
      "Epoch 85/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.5642 - accuracy: 0.8613 - val_loss: 0.8536 - val_accuracy: 0.7604\n",
      "Epoch 86/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 227ms/step - loss: 0.5621 - accuracy: 0.8627 - val_loss: 0.8488 - val_accuracy: 0.7690\n",
      "Epoch 87/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 90s 230ms/step - loss: 0.5621 - accuracy: 0.8613 - val_loss: 0.8764 - val_accuracy: 0.7684\n",
      "Epoch 88/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.5575 - accuracy: 0.8620 - val_loss: 0.8563 - val_accuracy: 0.7625\n",
      "Epoch 89/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.5621 - accuracy: 0.8610 - val_loss: 0.7749 - val_accuracy: 0.7918\n",
      "Epoch 90/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 90s 229ms/step - loss: 0.5550 - accuracy: 0.8648 - val_loss: 0.8070 - val_accuracy: 0.7926\n",
      "Epoch 91/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 226ms/step - loss: 0.5581 - accuracy: 0.8644 - val_loss: 0.8258 - val_accuracy: 0.7808\n",
      "Epoch 92/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 227ms/step - loss: 0.5525 - accuracy: 0.8641 - val_loss: 0.9186 - val_accuracy: 0.7521\n",
      "Epoch 93/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 90s 230ms/step - loss: 0.5514 - accuracy: 0.8656 - val_loss: 1.1730 - val_accuracy: 0.7048\n",
      "Epoch 94/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 90s 230ms/step - loss: 0.5557 - accuracy: 0.8652 - val_loss: 0.9430 - val_accuracy: 0.7583\n",
      "Epoch 95/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.5529 - accuracy: 0.8671 - val_loss: 0.8878 - val_accuracy: 0.7626\n",
      "Epoch 96/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 227ms/step - loss: 0.5491 - accuracy: 0.8683 - val_loss: 1.0014 - val_accuracy: 0.7352\n",
      "Epoch 97/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 90s 229ms/step - loss: 0.5505 - accuracy: 0.8667 - val_loss: 0.8600 - val_accuracy: 0.7688\n",
      "Epoch 98/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 229ms/step - loss: 0.5492 - accuracy: 0.8673 - val_loss: 0.8165 - val_accuracy: 0.7887\n",
      "Epoch 99/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 229ms/step - loss: 0.5438 - accuracy: 0.8677 - val_loss: 0.7881 - val_accuracy: 0.7874\n",
      "Epoch 100/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 226ms/step - loss: 0.5415 - accuracy: 0.8689 - val_loss: 1.1185 - val_accuracy: 0.7135\n",
      "Epoch 101/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.5476 - accuracy: 0.8671 - val_loss: 0.9627 - val_accuracy: 0.7447\n",
      "Epoch 102/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 90s 229ms/step - loss: 0.5447 - accuracy: 0.8670 - val_loss: 0.8796 - val_accuracy: 0.7634\n",
      "Epoch 103/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 229ms/step - loss: 0.5466 - accuracy: 0.8674 - val_loss: 1.0068 - val_accuracy: 0.7204\n",
      "Epoch 104/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.5434 - accuracy: 0.8702 - val_loss: 0.7324 - val_accuracy: 0.8043\n",
      "Epoch 105/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 90s 230ms/step - loss: 0.5408 - accuracy: 0.8691 - val_loss: 1.0082 - val_accuracy: 0.7322\n",
      "Epoch 106/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 227ms/step - loss: 0.5400 - accuracy: 0.8705 - val_loss: 0.7977 - val_accuracy: 0.7856\n",
      "Epoch 107/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 90s 230ms/step - loss: 0.5384 - accuracy: 0.8704 - val_loss: 1.3729 - val_accuracy: 0.6415\n",
      "Epoch 108/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 227ms/step - loss: 0.5392 - accuracy: 0.8725 - val_loss: 1.0230 - val_accuracy: 0.7338\n",
      "Epoch 109/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 90s 230ms/step - loss: 0.5395 - accuracy: 0.8702 - val_loss: 0.9321 - val_accuracy: 0.7532\n",
      "Epoch 110/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.5383 - accuracy: 0.8713 - val_loss: 0.9510 - val_accuracy: 0.7382\n",
      "Epoch 111/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 90s 229ms/step - loss: 0.5385 - accuracy: 0.8713 - val_loss: 0.8917 - val_accuracy: 0.7530\n",
      "Epoch 112/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.5371 - accuracy: 0.8698 - val_loss: 0.8808 - val_accuracy: 0.7604\n",
      "Epoch 113/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.5324 - accuracy: 0.8735 - val_loss: 1.1868 - val_accuracy: 0.6993\n",
      "Epoch 114/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 227ms/step - loss: 0.5348 - accuracy: 0.8717 - val_loss: 0.7276 - val_accuracy: 0.8114\n",
      "Epoch 115/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 229ms/step - loss: 0.5348 - accuracy: 0.8719 - val_loss: 0.9017 - val_accuracy: 0.7528\n",
      "Epoch 116/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.5315 - accuracy: 0.8744 - val_loss: 0.8254 - val_accuracy: 0.7762\n",
      "Epoch 117/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.5344 - accuracy: 0.8714 - val_loss: 1.0580 - val_accuracy: 0.7074\n",
      "Epoch 118/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 226ms/step - loss: 0.5257 - accuracy: 0.8748 - val_loss: 1.3356 - val_accuracy: 0.6708\n",
      "Epoch 119/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.5345 - accuracy: 0.8712 - val_loss: 1.0105 - val_accuracy: 0.7558\n",
      "Epoch 120/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.5296 - accuracy: 0.8744 - val_loss: 1.0027 - val_accuracy: 0.7362\n",
      "Epoch 121/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.5302 - accuracy: 0.8739 - val_loss: 0.7581 - val_accuracy: 0.8039\n",
      "Epoch 122/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.5268 - accuracy: 0.8751 - val_loss: 0.9332 - val_accuracy: 0.7549\n",
      "Epoch 123/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 90s 229ms/step - loss: 0.5224 - accuracy: 0.8780 - val_loss: 0.9822 - val_accuracy: 0.7437\n",
      "Epoch 124/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.5288 - accuracy: 0.8736 - val_loss: 1.5063 - val_accuracy: 0.6197\n",
      "Epoch 125/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 227ms/step - loss: 0.5244 - accuracy: 0.8771 - val_loss: 0.9782 - val_accuracy: 0.7352\n",
      "Epoch 126/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 90s 229ms/step - loss: 0.5249 - accuracy: 0.8763 - val_loss: 1.2190 - val_accuracy: 0.6885\n",
      "Epoch 127/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.5258 - accuracy: 0.8751 - val_loss: 0.9312 - val_accuracy: 0.7717\n",
      "Epoch 128/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 90s 231ms/step - loss: 0.5212 - accuracy: 0.8775 - val_loss: 0.8108 - val_accuracy: 0.7934\n",
      "Epoch 129/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 90s 229ms/step - loss: 0.5206 - accuracy: 0.8761 - val_loss: 0.9255 - val_accuracy: 0.7583\n",
      "Epoch 130/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 90s 229ms/step - loss: 0.5261 - accuracy: 0.8767 - val_loss: 0.8190 - val_accuracy: 0.7865\n",
      "Epoch 131/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 227ms/step - loss: 0.5232 - accuracy: 0.8769 - val_loss: 0.9557 - val_accuracy: 0.7539\n",
      "Epoch 132/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 90s 230ms/step - loss: 0.5203 - accuracy: 0.8778 - val_loss: 1.1371 - val_accuracy: 0.7156\n",
      "Epoch 133/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.5191 - accuracy: 0.8785 - val_loss: 0.8326 - val_accuracy: 0.7822\n",
      "Epoch 134/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 90s 230ms/step - loss: 0.5168 - accuracy: 0.8791 - val_loss: 0.9254 - val_accuracy: 0.7599\n",
      "Epoch 135/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 227ms/step - loss: 0.5188 - accuracy: 0.8788 - val_loss: 0.7278 - val_accuracy: 0.8157\n",
      "Epoch 136/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.5189 - accuracy: 0.8792 - val_loss: 0.9688 - val_accuracy: 0.7575\n",
      "Epoch 137/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 90s 229ms/step - loss: 0.5223 - accuracy: 0.8778 - val_loss: 0.8647 - val_accuracy: 0.7864\n",
      "Epoch 138/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 90s 230ms/step - loss: 0.5177 - accuracy: 0.8789 - val_loss: 0.8011 - val_accuracy: 0.7874\n",
      "Epoch 139/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.5159 - accuracy: 0.8784 - val_loss: 0.9575 - val_accuracy: 0.7541\n",
      "Epoch 140/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 90s 230ms/step - loss: 0.5223 - accuracy: 0.8766 - val_loss: 0.8761 - val_accuracy: 0.7761\n",
      "Epoch 141/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 90s 231ms/step - loss: 0.5182 - accuracy: 0.8782 - val_loss: 0.8531 - val_accuracy: 0.7816\n",
      "Epoch 142/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 90s 231ms/step - loss: 0.5168 - accuracy: 0.8802 - val_loss: 0.9154 - val_accuracy: 0.7573\n",
      "Epoch 143/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 90s 229ms/step - loss: 0.5152 - accuracy: 0.8792 - val_loss: 0.9504 - val_accuracy: 0.7525\n",
      "Epoch 144/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 90s 231ms/step - loss: 0.5150 - accuracy: 0.8797 - val_loss: 0.8559 - val_accuracy: 0.7758\n",
      "Epoch 145/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.5141 - accuracy: 0.8796 - val_loss: 0.9819 - val_accuracy: 0.7475\n",
      "Epoch 146/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 90s 230ms/step - loss: 0.5130 - accuracy: 0.8812 - val_loss: 0.9444 - val_accuracy: 0.7431\n",
      "Epoch 147/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 90s 230ms/step - loss: 0.5134 - accuracy: 0.8784 - val_loss: 0.9206 - val_accuracy: 0.7672\n",
      "Epoch 148/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 90s 229ms/step - loss: 0.5140 - accuracy: 0.8802 - val_loss: 1.0030 - val_accuracy: 0.7318\n",
      "Epoch 149/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.5133 - accuracy: 0.8814 - val_loss: 0.9620 - val_accuracy: 0.7481\n",
      "Epoch 150/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.5120 - accuracy: 0.8814 - val_loss: 0.7389 - val_accuracy: 0.8229\n",
      "Epoch 151/250\n",
      "Learning rate:  0.1\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.5155 - accuracy: 0.8800 - val_loss: 0.9823 - val_accuracy: 0.7415\n",
      "Epoch 152/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 90s 230ms/step - loss: 0.4202 - accuracy: 0.9136 - val_loss: 0.6418 - val_accuracy: 0.8384\n",
      "Epoch 153/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 89s 227ms/step - loss: 0.3885 - accuracy: 0.9243 - val_loss: 0.6238 - val_accuracy: 0.8448\n",
      "Epoch 154/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 89s 227ms/step - loss: 0.3779 - accuracy: 0.9265 - val_loss: 0.6660 - val_accuracy: 0.8300\n",
      "Epoch 155/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 90s 230ms/step - loss: 0.3675 - accuracy: 0.9311 - val_loss: 0.6410 - val_accuracy: 0.8363\n",
      "Epoch 156/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 90s 230ms/step - loss: 0.3651 - accuracy: 0.9297 - val_loss: 0.6460 - val_accuracy: 0.8376\n",
      "Epoch 157/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 89s 229ms/step - loss: 0.3556 - accuracy: 0.9316 - val_loss: 0.6276 - val_accuracy: 0.8435\n",
      "Epoch 158/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.3532 - accuracy: 0.9318 - val_loss: 0.6295 - val_accuracy: 0.8420\n",
      "Epoch 159/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 89s 229ms/step - loss: 0.3473 - accuracy: 0.9333 - val_loss: 0.6260 - val_accuracy: 0.8408\n",
      "Epoch 160/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 89s 229ms/step - loss: 0.3416 - accuracy: 0.9341 - val_loss: 0.6665 - val_accuracy: 0.8291\n",
      "Epoch 161/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.3357 - accuracy: 0.9356 - val_loss: 0.6566 - val_accuracy: 0.8343\n",
      "Epoch 162/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 89s 227ms/step - loss: 0.3322 - accuracy: 0.9358 - val_loss: 0.6150 - val_accuracy: 0.8456\n",
      "Epoch 163/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 90s 230ms/step - loss: 0.3281 - accuracy: 0.9364 - val_loss: 0.6153 - val_accuracy: 0.8459\n",
      "Epoch 164/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 90s 229ms/step - loss: 0.3256 - accuracy: 0.9364 - val_loss: 0.6286 - val_accuracy: 0.8396\n",
      "Epoch 165/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 90s 229ms/step - loss: 0.3220 - accuracy: 0.9368 - val_loss: 0.6223 - val_accuracy: 0.8427\n",
      "Epoch 166/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 94s 241ms/step - loss: 0.3198 - accuracy: 0.9378 - val_loss: 0.5991 - val_accuracy: 0.8469\n",
      "Epoch 167/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 89s 227ms/step - loss: 0.3151 - accuracy: 0.9385 - val_loss: 0.6437 - val_accuracy: 0.8354\n",
      "Epoch 168/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 89s 229ms/step - loss: 0.3108 - accuracy: 0.9390 - val_loss: 0.5868 - val_accuracy: 0.8491\n",
      "Epoch 169/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 90s 229ms/step - loss: 0.3055 - accuracy: 0.9398 - val_loss: 0.5942 - val_accuracy: 0.8485\n",
      "Epoch 170/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.3054 - accuracy: 0.9405 - val_loss: 0.6635 - val_accuracy: 0.8320\n",
      "Epoch 171/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.3007 - accuracy: 0.9414 - val_loss: 0.6088 - val_accuracy: 0.8444\n",
      "Epoch 172/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.2994 - accuracy: 0.9401 - val_loss: 0.6594 - val_accuracy: 0.8308\n",
      "Epoch 173/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.2960 - accuracy: 0.9408 - val_loss: 0.6269 - val_accuracy: 0.8391\n",
      "Epoch 174/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 90s 229ms/step - loss: 0.2942 - accuracy: 0.9416 - val_loss: 0.6178 - val_accuracy: 0.8418\n",
      "Epoch 175/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.2921 - accuracy: 0.9408 - val_loss: 0.6138 - val_accuracy: 0.8407\n",
      "Epoch 176/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 89s 227ms/step - loss: 0.2858 - accuracy: 0.9432 - val_loss: 0.6324 - val_accuracy: 0.8350\n",
      "Epoch 177/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 90s 231ms/step - loss: 0.2871 - accuracy: 0.9417 - val_loss: 0.6072 - val_accuracy: 0.8437\n",
      "Epoch 178/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.2832 - accuracy: 0.9430 - val_loss: 0.6300 - val_accuracy: 0.8348\n",
      "Epoch 179/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.2818 - accuracy: 0.9418 - val_loss: 0.6527 - val_accuracy: 0.8309\n",
      "Epoch 180/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.2814 - accuracy: 0.9431 - val_loss: 0.6282 - val_accuracy: 0.8377\n",
      "Epoch 181/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 90s 230ms/step - loss: 0.2793 - accuracy: 0.9423 - val_loss: 0.5923 - val_accuracy: 0.8473\n",
      "Epoch 182/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 90s 230ms/step - loss: 0.2755 - accuracy: 0.9444 - val_loss: 0.6445 - val_accuracy: 0.8351\n",
      "Epoch 183/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.2751 - accuracy: 0.9425 - val_loss: 0.6229 - val_accuracy: 0.8432\n",
      "Epoch 184/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.2696 - accuracy: 0.9440 - val_loss: 0.6124 - val_accuracy: 0.8456\n",
      "Epoch 185/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.2729 - accuracy: 0.9428 - val_loss: 0.6067 - val_accuracy: 0.8426\n",
      "Epoch 186/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 89s 229ms/step - loss: 0.2703 - accuracy: 0.9428 - val_loss: 0.6377 - val_accuracy: 0.8322\n",
      "Epoch 187/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 90s 230ms/step - loss: 0.2645 - accuracy: 0.9445 - val_loss: 0.6361 - val_accuracy: 0.8300\n",
      "Epoch 188/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 89s 229ms/step - loss: 0.2696 - accuracy: 0.9427 - val_loss: 0.6275 - val_accuracy: 0.8325\n",
      "Epoch 189/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 90s 229ms/step - loss: 0.2635 - accuracy: 0.9454 - val_loss: 0.5688 - val_accuracy: 0.8523\n",
      "Epoch 190/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 88s 226ms/step - loss: 0.2633 - accuracy: 0.9449 - val_loss: 0.5830 - val_accuracy: 0.8480\n",
      "Epoch 191/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 90s 230ms/step - loss: 0.2596 - accuracy: 0.9456 - val_loss: 0.6222 - val_accuracy: 0.8393\n",
      "Epoch 192/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 90s 229ms/step - loss: 0.2625 - accuracy: 0.9437 - val_loss: 0.5970 - val_accuracy: 0.8468\n",
      "Epoch 193/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 90s 229ms/step - loss: 0.2569 - accuracy: 0.9460 - val_loss: 0.6101 - val_accuracy: 0.8390\n",
      "Epoch 194/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.2582 - accuracy: 0.9443 - val_loss: 0.5860 - val_accuracy: 0.8476\n",
      "Epoch 195/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 90s 229ms/step - loss: 0.2543 - accuracy: 0.9452 - val_loss: 0.5690 - val_accuracy: 0.8536\n",
      "Epoch 196/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 89s 229ms/step - loss: 0.2535 - accuracy: 0.9450 - val_loss: 0.6152 - val_accuracy: 0.8372\n",
      "Epoch 197/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 90s 229ms/step - loss: 0.2566 - accuracy: 0.9433 - val_loss: 0.6350 - val_accuracy: 0.8279\n",
      "Epoch 198/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 89s 229ms/step - loss: 0.2550 - accuracy: 0.9458 - val_loss: 0.6380 - val_accuracy: 0.8320\n",
      "Epoch 199/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 90s 229ms/step - loss: 0.2548 - accuracy: 0.9440 - val_loss: 0.5608 - val_accuracy: 0.8489\n",
      "Epoch 200/250\n",
      "Learning rate:  0.01\n",
      "391/391 [==============================] - 88s 226ms/step - loss: 0.2527 - accuracy: 0.9444 - val_loss: 0.6357 - val_accuracy: 0.8333\n",
      "Epoch 201/250\n",
      "Learning rate:  0.001\n",
      "391/391 [==============================] - 90s 231ms/step - loss: 0.2334 - accuracy: 0.9514 - val_loss: 0.5829 - val_accuracy: 0.8457\n",
      "Epoch 202/250\n",
      "Learning rate:  0.001\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.2263 - accuracy: 0.9546 - val_loss: 0.5799 - val_accuracy: 0.8472\n",
      "Epoch 203/250\n",
      "Learning rate:  0.001\n",
      "391/391 [==============================] - 90s 230ms/step - loss: 0.2226 - accuracy: 0.9558 - val_loss: 0.5575 - val_accuracy: 0.8544\n",
      "Epoch 204/250\n",
      "Learning rate:  0.001\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.2193 - accuracy: 0.9576 - val_loss: 0.5684 - val_accuracy: 0.8518\n",
      "Epoch 205/250\n",
      "Learning rate:  0.001\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.2182 - accuracy: 0.9583 - val_loss: 0.5646 - val_accuracy: 0.8508\n",
      "Epoch 206/250\n",
      "Learning rate:  0.001\n",
      "391/391 [==============================] - 90s 230ms/step - loss: 0.2197 - accuracy: 0.9567 - val_loss: 0.5695 - val_accuracy: 0.8512\n",
      "Epoch 207/250\n",
      "Learning rate:  0.001\n",
      "391/391 [==============================] - 90s 231ms/step - loss: 0.2173 - accuracy: 0.9574 - val_loss: 0.5681 - val_accuracy: 0.8494\n",
      "Epoch 208/250\n",
      "Learning rate:  0.001\n",
      "391/391 [==============================] - 88s 226ms/step - loss: 0.2172 - accuracy: 0.9577 - val_loss: 0.5661 - val_accuracy: 0.8487\n",
      "Epoch 209/250\n",
      "Learning rate:  0.001\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.2146 - accuracy: 0.9584 - val_loss: 0.5697 - val_accuracy: 0.8529\n",
      "Epoch 210/250\n",
      "Learning rate:  0.001\n",
      "391/391 [==============================] - 90s 230ms/step - loss: 0.2136 - accuracy: 0.9590 - val_loss: 0.5816 - val_accuracy: 0.8460\n",
      "Epoch 211/250\n",
      "Learning rate:  0.001\n",
      "391/391 [==============================] - 90s 230ms/step - loss: 0.2126 - accuracy: 0.9587 - val_loss: 0.5661 - val_accuracy: 0.8527\n",
      "Epoch 212/250\n",
      "Learning rate:  0.001\n",
      "391/391 [==============================] - 90s 229ms/step - loss: 0.2119 - accuracy: 0.9590 - val_loss: 0.5719 - val_accuracy: 0.8525\n",
      "Epoch 213/250\n",
      "Learning rate:  0.001\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.2121 - accuracy: 0.9596 - val_loss: 0.5793 - val_accuracy: 0.8469\n",
      "Epoch 214/250\n",
      "Learning rate:  0.001\n",
      "391/391 [==============================] - 89s 229ms/step - loss: 0.2096 - accuracy: 0.9594 - val_loss: 0.5877 - val_accuracy: 0.8450\n",
      "Epoch 215/250\n",
      "Learning rate:  0.001\n",
      "391/391 [==============================] - 90s 230ms/step - loss: 0.2124 - accuracy: 0.9590 - val_loss: 0.5833 - val_accuracy: 0.8467\n",
      "Epoch 216/250\n",
      "Learning rate:  0.001\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.2137 - accuracy: 0.9584 - val_loss: 0.5924 - val_accuracy: 0.8470\n",
      "Epoch 217/250\n",
      "Learning rate:  0.001\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.2112 - accuracy: 0.9597 - val_loss: 0.5653 - val_accuracy: 0.8523\n",
      "Epoch 218/250\n",
      "Learning rate:  0.001\n",
      "391/391 [==============================] - 89s 227ms/step - loss: 0.2108 - accuracy: 0.9595 - val_loss: 0.5800 - val_accuracy: 0.8482\n",
      "Epoch 219/250\n",
      "Learning rate:  0.001\n",
      "391/391 [==============================] - 90s 230ms/step - loss: 0.2122 - accuracy: 0.9589 - val_loss: 0.5780 - val_accuracy: 0.8489\n",
      "Epoch 220/250\n",
      "Learning rate:  0.001\n",
      "391/391 [==============================] - 90s 229ms/step - loss: 0.2098 - accuracy: 0.9604 - val_loss: 0.5740 - val_accuracy: 0.8523\n",
      "Epoch 221/250\n",
      "Learning rate:  0.001\n",
      "391/391 [==============================] - 90s 231ms/step - loss: 0.2092 - accuracy: 0.9608 - val_loss: 0.5855 - val_accuracy: 0.8465\n",
      "Epoch 222/250\n",
      "Learning rate:  0.001\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.2105 - accuracy: 0.9589 - val_loss: 0.5846 - val_accuracy: 0.8465\n",
      "Epoch 223/250\n",
      "Learning rate:  0.001\n",
      "391/391 [==============================] - 90s 231ms/step - loss: 0.2096 - accuracy: 0.9595 - val_loss: 0.5644 - val_accuracy: 0.8521\n",
      "Epoch 224/250\n",
      "Learning rate:  0.001\n",
      "391/391 [==============================] - 90s 230ms/step - loss: 0.2090 - accuracy: 0.9604 - val_loss: 0.5622 - val_accuracy: 0.8514\n",
      "Epoch 225/250\n",
      "Learning rate:  0.001\n",
      "391/391 [==============================] - 90s 231ms/step - loss: 0.2083 - accuracy: 0.9595 - val_loss: 0.5797 - val_accuracy: 0.8496\n",
      "Epoch 226/250\n",
      "Learning rate:  0.001\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.2084 - accuracy: 0.9598 - val_loss: 0.5777 - val_accuracy: 0.8492\n",
      "Epoch 227/250\n",
      "Learning rate:  0.001\n",
      "391/391 [==============================] - 91s 232ms/step - loss: 0.2038 - accuracy: 0.9617 - val_loss: 0.5849 - val_accuracy: 0.8487\n",
      "Epoch 228/250\n",
      "Learning rate:  0.001\n",
      "391/391 [==============================] - 90s 231ms/step - loss: 0.2073 - accuracy: 0.9598 - val_loss: 0.5857 - val_accuracy: 0.8466\n",
      "Epoch 229/250\n",
      "Learning rate:  0.001\n",
      "391/391 [==============================] - 90s 231ms/step - loss: 0.2071 - accuracy: 0.9603 - val_loss: 0.5777 - val_accuracy: 0.8497\n",
      "Epoch 230/250\n",
      "Learning rate:  0.001\n",
      "391/391 [==============================] - 90s 230ms/step - loss: 0.2060 - accuracy: 0.9612 - val_loss: 0.5724 - val_accuracy: 0.8505\n",
      "Epoch 231/250\n",
      "Learning rate:  0.001\n",
      "391/391 [==============================] - 90s 231ms/step - loss: 0.2056 - accuracy: 0.9603 - val_loss: 0.5800 - val_accuracy: 0.8482\n",
      "Epoch 232/250\n",
      "Learning rate:  0.001\n",
      "391/391 [==============================] - 90s 230ms/step - loss: 0.2062 - accuracy: 0.9610 - val_loss: 0.5781 - val_accuracy: 0.8496\n",
      "Epoch 233/250\n",
      "Learning rate:  0.001\n",
      "391/391 [==============================] - 91s 232ms/step - loss: 0.2051 - accuracy: 0.9608 - val_loss: 0.5859 - val_accuracy: 0.8483\n",
      "Epoch 234/250\n",
      "Learning rate:  0.001\n",
      "391/391 [==============================] - 91s 232ms/step - loss: 0.2053 - accuracy: 0.9606 - val_loss: 0.5830 - val_accuracy: 0.8497\n",
      "Epoch 235/250\n",
      "Learning rate:  0.001\n",
      "391/391 [==============================] - 90s 230ms/step - loss: 0.2064 - accuracy: 0.9601 - val_loss: 0.5719 - val_accuracy: 0.8501\n",
      "Epoch 236/250\n",
      "Learning rate:  0.001\n",
      "391/391 [==============================] - 90s 230ms/step - loss: 0.2047 - accuracy: 0.9607 - val_loss: 0.5834 - val_accuracy: 0.8484\n",
      "Epoch 237/250\n",
      "Learning rate:  0.001\n",
      "391/391 [==============================] - 90s 230ms/step - loss: 0.2051 - accuracy: 0.9596 - val_loss: 0.5810 - val_accuracy: 0.8483\n",
      "Epoch 238/250\n",
      "Learning rate:  0.001\n",
      "391/391 [==============================] - 89s 229ms/step - loss: 0.2023 - accuracy: 0.9615 - val_loss: 0.5885 - val_accuracy: 0.8458\n",
      "Epoch 239/250\n",
      "Learning rate:  0.001\n",
      "391/391 [==============================] - 90s 230ms/step - loss: 0.2024 - accuracy: 0.9613 - val_loss: 0.5733 - val_accuracy: 0.8516\n",
      "Epoch 240/250\n",
      "Learning rate:  0.001\n",
      "391/391 [==============================] - 90s 229ms/step - loss: 0.2009 - accuracy: 0.9627 - val_loss: 0.5815 - val_accuracy: 0.8499\n",
      "Epoch 241/250\n",
      "Learning rate:  0.001\n",
      "391/391 [==============================] - 90s 231ms/step - loss: 0.2013 - accuracy: 0.9608 - val_loss: 0.5839 - val_accuracy: 0.8483\n",
      "Epoch 242/250\n",
      "Learning rate:  0.001\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.2005 - accuracy: 0.9623 - val_loss: 0.5808 - val_accuracy: 0.8492\n",
      "Epoch 243/250\n",
      "Learning rate:  0.001\n",
      "391/391 [==============================] - 90s 231ms/step - loss: 0.2029 - accuracy: 0.9614 - val_loss: 0.5870 - val_accuracy: 0.8481\n",
      "Epoch 244/250\n",
      "Learning rate:  0.001\n",
      "391/391 [==============================] - 89s 229ms/step - loss: 0.2030 - accuracy: 0.9609 - val_loss: 0.5890 - val_accuracy: 0.8464\n",
      "Epoch 245/250\n",
      "Learning rate:  0.001\n",
      "391/391 [==============================] - 90s 229ms/step - loss: 0.2027 - accuracy: 0.9610 - val_loss: 0.5813 - val_accuracy: 0.8498\n",
      "Epoch 246/250\n",
      "Learning rate:  0.001\n",
      "391/391 [==============================] - 90s 231ms/step - loss: 0.2026 - accuracy: 0.9605 - val_loss: 0.5833 - val_accuracy: 0.8492\n",
      "Epoch 247/250\n",
      "Learning rate:  0.001\n",
      "391/391 [==============================] - 91s 232ms/step - loss: 0.2014 - accuracy: 0.9611 - val_loss: 0.5699 - val_accuracy: 0.8529\n",
      "Epoch 248/250\n",
      "Learning rate:  0.001\n",
      "391/391 [==============================] - 90s 230ms/step - loss: 0.1977 - accuracy: 0.9629 - val_loss: 0.5868 - val_accuracy: 0.8495\n",
      "Epoch 249/250\n",
      "Learning rate:  0.001\n",
      "391/391 [==============================] - 88s 226ms/step - loss: 0.2008 - accuracy: 0.9607 - val_loss: 0.5809 - val_accuracy: 0.8503\n",
      "Epoch 250/250\n",
      "Learning rate:  0.001\n",
      "391/391 [==============================] - 90s 230ms/step - loss: 0.1993 - accuracy: 0.9625 - val_loss: 0.5749 - val_accuracy: 0.8518\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff14c17ec10>"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 250\n",
    "\n",
    "tess_resnet.fit(datagen.flow(Xtr_rgb, ytr_cifar, batch_size=batch_size), epochs=epochs, validation_data=(Xte_rgb,yte_cifar), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WzCJcwQ-XmV7",
    "outputId": "7d201eb6-ff6f-47f7-94f8-2a0e34e4c52b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "tess_conv2d (TessConv2D)     (None, None, None, 24)    216       \n",
      "_________________________________________________________________\n",
      "tess_residual_unit (TessResi (None, None, None, 24)    2928      \n",
      "_________________________________________________________________\n",
      "tess_residual_unit_1 (TessRe (None, None, None, 24)    2928      \n",
      "_________________________________________________________________\n",
      "tess_residual_unit_2 (TessRe (None, None, None, 24)    2928      \n",
      "_________________________________________________________________\n",
      "tess_residual_unit_3 (TessRe (None, None, None, 48)    9072      \n",
      "_________________________________________________________________\n",
      "tess_residual_unit_4 (TessRe (None, None, None, 48)    11040     \n",
      "_________________________________________________________________\n",
      "tess_residual_unit_5 (TessRe (None, None, None, 96)    34272     \n",
      "_________________________________________________________________\n",
      "tess_residual_unit_6 (TessRe (None, None, None, 96)    42816     \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                970       \n",
      "=================================================================\n",
      "Total params: 107,170\n",
      "Trainable params: 104,146\n",
      "Non-trainable params: 3,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tess_resnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ABJNjy294yw",
    "outputId": "4c6f69d1-9299-4d5d-ecb5-b35131061252"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96594"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(np.argmax(ytr_cifar, axis=1),np.argmax(tess_resnet.predict(Xtr_rgb),axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SuzVsBHq-Jxq",
    "outputId": "b74a5a09-5112-4cdd-e058-8203d71fb5d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8518"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(np.argmax(yte_cifar, axis=1),np.argmax(tess_resnet.predict(Xte_rgb),axis=1))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Tess_ValidTest3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
